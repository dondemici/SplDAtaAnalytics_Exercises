{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12839f79-6be8-441f-893a-26e08ed0d9f7",
   "metadata": {},
   "source": [
    "# CSIS4260 — Linear Regression: From Fundamentals to Evaluation\n",
    "#### Lecture 9, Ch - 14, 15\n",
    "##### Based on \"Data Science from Scratch, 2nd Edition\" by Joel Grus\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction: Scenario & Motivation\n",
    "\n",
    "Suppose you’re a data scientist at a fintech startup, tasked with predicting a customer’s annual spending based on features like age and income. Before we approach regression, let’s recall tools from last class—clustering and apriori (association rule mining)—and why those are insufficient when the outcome is continuous, not categorical or binary.\n",
    "\n",
    "Recall:  \n",
    "- Clustering is unsupervised: finds groupings, doesn’t predict values.\n",
    "- Apriori finds associations (e.g., \"If {milk, bread}, then {butter}\"), works with categorical/basket data, not for forecasting.\n",
    "\n",
    "Problem:  \n",
    "When the target is numeric (e.g., salary, price, grade), we need a regression approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631fd0a1-f76a-4127-aefb-65c6ce08ddcc",
   "metadata": {},
   "source": [
    "## Revisiting Previous Methods: Clustering and Apriori\n",
    "\n",
    "Before we dive into regression, let’s briefly connect to what you’ve learned so far:\n",
    "\n",
    "- **Clustering**: Unsupervised learning technique for discovering groups or patterns in data. It does not provide a way to predict a numeric target variable.\n",
    "- **Apriori (Association Rule Mining)**: Used to find associations or frequent patterns in categorical data, such as items frequently purchased together in transactions. These rules help uncover relationships but do not predict continuous outcomes.\n",
    "\n",
    "Both methods are valuable for understanding structure and relationships in data, but when our goal is to predict or explain a numeric quantity, we need a fundamentally different approach: regression.\n",
    "\n",
    "---\n",
    "\n",
    "In this lecture, we will transition from grouping and association-based methods to **regression analysis**—the foundation for predictive modeling with continuous outcomes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d5fa9b-effb-4d05-9f55-b90e41b50190",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "### Definition\n",
    "Linear regression models the relationship between one or more independent variables and a continuous dependent variable, assuming linearity.\n",
    "\n",
    "Linear regression estimates the expected value of a continuous dependent variable as a linear function of the independent variables.\n",
    "\n",
    "### Mathematical Formula\n",
    "\n",
    "**Simple Linear Regression:**\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x + \\epsilon$$\n",
    "\n",
    "**Multiple Linear Regression:**\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n + \\epsilon$$\n",
    "\n",
    "- $y$: Dependent variable\n",
    "- $x_i$: Independent variables\n",
    "- $\\beta_0$: Intercept\n",
    "- $\\beta_i$: Coefficients\n",
    "- $\\epsilon$: Error (residual)\n",
    "\n",
    "### Explaining the Formula\n",
    "\n",
    "- **Intercept** ($\\beta_0$): Value when all $x$ are zero.\n",
    "- **Coefficients** ($\\beta_i$): Change in $y$ per unit increase in $x_i$.\n",
    "- **Error** ($\\epsilon$): Difference between actual and predicted $y$.\n",
    "\n",
    "\n",
    "## Understanding Linear Regression: Key Concepts & Definitions\n",
    "\n",
    "**Linear regression** is a statistical technique that models the relationship between a dependent variable (also called the response or outcome variable) and one or more independent variables (also called predictors, features, or explanatory variables). The goal is to fit a line (or in higher dimensions, a hyperplane) that best predicts the outcome based on the inputs.\n",
    "\n",
    "### Key Terms:\n",
    "\n",
    "- **Dependent Variable ($y$)**: The outcome you are trying to predict or explain (e.g., house price).\n",
    "\n",
    "- **Independent Variable ($x$ or $x_i$)**: The input or predictor used to explain changes in the dependent variable (e.g., living area in square feet).\n",
    "\n",
    "- **Regression Line/Equation**: The mathematical relationship is typically written as:\n",
    "  $$\n",
    "  y = \\beta_0 + \\beta_1 x + \\epsilon\n",
    "  $$\n",
    "  - $y$: predicted value of the dependent variable  \n",
    "  - $x$: value of the independent variable  \n",
    "  - $\\beta_0$: intercept (the value of $y$ when $x = 0$)  \n",
    "  - $\\beta_1$: slope (how much $y$ increases for a one-unit increase in $x$)  \n",
    "  - $\\epsilon$: error or residual (difference between observed and predicted value)  \n",
    "\n",
    "- **Slope ($\\beta_1$)**: Measures the change in the dependent variable for a one-unit increase in the independent variable. In the equation, it is the coefficient of $x$.\n",
    "\n",
    "- **Intercept ($\\beta_0$)**: The expected value of the dependent variable when all independent variables are zero. Graphically, it is where the regression line crosses the $y$-axis.\n",
    "\n",
    "- **Residuals ($\\epsilon$)**: The difference between the actual observed value and the value predicted by the regression model for each observation. Residuals are used to diagnose the fit and assumptions of the model.\n",
    "\n",
    "- **Ordinary Least Squares (OLS)**: The most common method to estimate the regression coefficients. OLS chooses the line that minimizes the sum of the squared residuals.\n",
    "\n",
    "### Main Assumptions of Linear Regression:\n",
    "\n",
    "1. **Linearity**: The relationship between the independent and dependent variable is linear.\n",
    "2. **Independence**: The residuals (errors) are independent.\n",
    "3. **Homoscedasticity**: The residuals have constant variance at all levels of the independent variable.\n",
    "4. **Normality**: The residuals are normally distributed.\n",
    "\n",
    "**Why regression?**  \n",
    "Regression allows us to quantify the relationship between variables, make predictions, and infer which variables are most strongly associated with the outcome.\n",
    "\n",
    "\n",
    "## Example: Predicting Home Prices\n",
    "\n",
    "| Living Area (sq ft) | Price ($1000s) |\n",
    "|---------------------|----------------|\n",
    "| 1400                | 245            |\n",
    "| 1600                | 312            |\n",
    "| 1700                | 279            |\n",
    "| 1875                | 308            |\n",
    "| 1100                | 199            |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b387f9-3279-4c1d-81d0-73fc5d4f8005",
   "metadata": {},
   "source": [
    "## Step-by-Step: Manual Linear Regression Calculation\n",
    "\n",
    "### OLS Formulas:\n",
    "\n",
    "**Slope $(\\beta_1)$:**\n",
    "$$\n",
    "\\beta_1 = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\n",
    "$$\n",
    "\n",
    "**Intercept $(\\beta_0)$:**\n",
    "$$\n",
    "\\beta_0 = \\bar{y} - \\beta_1\\bar{x}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Calculate the Means\n",
    "\n",
    "- Mean of living area $(\\bar{x}$):\n",
    "$$\n",
    "\\bar{x} = \\frac{1400 + 1600 + 1700 + 1875 + 1100}{5} = \\frac{7675}{5} = 1535\n",
    "$$\n",
    "\n",
    "- Mean of price $(\\bar{y}$):\n",
    "$$\n",
    "\\bar{y} = \\frac{245 + 312 + 279 + 308 + 199}{5} = \\frac{1343}{5} = 268.6\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Calculate $(\\beta_1)$\n",
    "\n",
    "Compute each component separately:\n",
    "\n",
    "| $(x_i)$ | $(y_i)$ | $(x_i - \\bar{x})$ | $(y_i - \\bar{y})$ | $((x_i - \\bar{x})(y_i - \\bar{y}))$ | $((x_i - \\bar{x})^2)$ |\n",
    "|---------|---------|-------------------|-------------------|-------------------------------------|-----------------------|\n",
    "| 1400    | 245     | -135              | -23.6             | 3186                                | 18225                 |\n",
    "| 1600    | 312     | 65                | 43.4              | 2821                                | 4225                  |\n",
    "| 1700    | 279     | 165               | 10.4              | 1716                                | 27225                 |\n",
    "| 1875    | 308     | 340               | 39.4              | 13396                               | 115600                |\n",
    "| 1100    | 199     | -435              | -69.6             | 30276                               | 189225                |\n",
    "\n",
    "**Summations:**\n",
    "\n",
    "- Sum of $((x_i - \\bar{x})(y_i - \\bar{y})$:\n",
    "$$\n",
    "3186 + 2821 + 1716 + 13396 + 30276 = 51395\n",
    "$$\n",
    "\n",
    "- Sum of $(x_i - \\bar{x})^2)$:\n",
    "$$\n",
    "18225 + 4225 + 27225 + 115600 + 189225 = 334500\n",
    "$$\n",
    "\n",
    "Calculate the slope $(\\beta_1)$:\n",
    "$$\n",
    "\\beta_1 = \\frac{51395}{334500} \\approx 0.1537\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Calculate $(\\beta_0)$\n",
    "\n",
    "Using the calculated slope $(\\beta_1)$:\n",
    "$$\n",
    "\\beta_0 = \\bar{y} - \\beta_1\\bar{x} = 268.6 - (0.1537 \\times 1535) \\approx 268.6 - 236.12 = 32.48\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Final Regression Equation:\n",
    "\n",
    "The resulting linear regression equation is:\n",
    "$$\n",
    "y = 32.48 + 0.1537x\n",
    "$$\n",
    "\n",
    "**Interpretation:**  \n",
    "This equation predicts home prices (in thousands of dollars) based on the living area (in square feet). For each additional square foot, the price is expected to increase by approximately \\$153.70.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ecc017-c31e-4c37-ae29-d74005a010c5",
   "metadata": {},
   "source": [
    "## Book Example: Step-by-Step Calculation\n",
    "\n",
    "Suppose we have this dataset (from the book):\n",
    "\n",
    "| $x$ | $y$ |\n",
    "|-----|-----|\n",
    "| 1   | 2   |\n",
    "| 2   | 4   |\n",
    "| 3   | 5   |\n",
    "| 4   | 4   |\n",
    "| 5   | 5   |\n",
    "\n",
    "We want to fit a line $y = \\beta_0 + \\beta_1 x$.\n",
    "\n",
    "### Step 1: Compute the means\n",
    "- $\\bar{x} = (1+2+3+4+5) / 5 = 3$\n",
    "- $\\bar{y} = (2+4+5+4+5) / 5 = 4$\n",
    "\n",
    "### Step 2: Compute the slope ($\\beta_1$)\n",
    "$$\n",
    "\\beta_1 = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\n",
    "$$\n",
    "\n",
    "Calculate each term:\n",
    "- $(1-3)(2-4) = (-2) \\times (-2) = 4$\n",
    "- $(2-3)(4-4) = (-1) \\times (0) = 0$\n",
    "- $(3-3)(5-4) = 0 \\times 1 = 0$\n",
    "- $(4-3)(4-4) = 1 \\times 0 = 0$\n",
    "- $(5-3)(5-4) = 2 \\times 1 = 2$\n",
    "\n",
    "Sum: $4 + 0 + 0 + 0 + 2 = 6$\n",
    "\n",
    "Denominator:\n",
    "- $(1-3)^2 = 4$\n",
    "- $(2-3)^2 = 1$\n",
    "- $(3-3)^2 = 0$\n",
    "- $(4-3)^2 = 1$\n",
    "- $(5-3)^2 = 4$\n",
    "\n",
    "Sum: $4 + 1 + 0 + 1 + 4 = 10$\n",
    "\n",
    "So, $\\beta_1 = 6 / 10 = 0.6$\n",
    "\n",
    "### Step 3: Compute the intercept ($\\beta_0$)\n",
    "$$\n",
    "\\beta_0 = \\bar{y} - \\beta_1 \\bar{x} = 4 - 0.6 \\times 3 = 2.2\n",
    "$$\n",
    "\n",
    "**Final regression equation:**  \n",
    "$$\n",
    "y = 2.2 + 0.6x\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "This fitted line describes the best linear relationship between $x$ and $y$ in the dataset above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6970d7df-dbf4-4298-9b04-ed9ed79904bd",
   "metadata": {},
   "source": [
    "## Step-by-Step Linear Regression with sklearn Dataset\n",
    "\n",
    "We will use the California Housing dataset, fit a linear regression model, show predictions, and plot the residuals.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5247acaf-5512-4e41-ac5e-2dcff2bb6e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load California housing data\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "X = data.data  # Use all variables\n",
    "y = data.target  # Median house value in $100,000s\n",
    "\n",
    "print(\"Variables:\", list(X.columns))\n",
    "df = pd.concat([X, y], axis=1)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a101d87-84b1-423a-82c0-0be0646e42a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f191df28-c4f9-411a-8188-1764b730be49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "import plotly.colors\n",
    "\n",
    "num_cols = df.select_dtypes(include='number').columns\n",
    "colors = plotly.colors.qualitative.Plotly  # 10 unique, cycling if >10 vars\n",
    "\n",
    "for i, col in enumerate(num_cols):\n",
    "    data = [df[col].dropna()]\n",
    "    group_labels = [col]\n",
    "    color = colors[i % len(colors)]\n",
    "    fig = ff.create_distplot(\n",
    "        data, group_labels, \n",
    "        bin_size=(df[col].max() - df[col].min()) / 20,\n",
    "        show_hist=True, show_curve=True, show_rug=False,\n",
    "        colors=[color]\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        title_text=f'Histogram and KDE for {col}',\n",
    "        template='plotly_white',\n",
    "        xaxis_title=col,\n",
    "        yaxis_title=\"Density\",\n",
    "        width=700,\n",
    "        height=400,\n",
    "        title_x=0.5\n",
    "    )\n",
    "    fig.update_traces(marker_line_color='black', marker_line_width=1)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a62852-0d9b-4bd1-8268-090625af39b6",
   "metadata": {},
   "source": [
    "### Understanding the Value of the KDE (Kernel Density Estimate) Line\n",
    "\n",
    "While histograms show the frequency distribution of values in discrete bins, the **KDE (Kernel Density Estimate) line** provides a smoothed, continuous approximation of the data’s underlying probability density function. The KDE helps in several ways:\n",
    "\n",
    "- **Smooths Out Noise:** Unlike histograms, which depend on bin edges and can appear jagged, the KDE line reveals the overall shape of the distribution without being sensitive to bin width.\n",
    "- **Reveals Structure:** It can make it easier to see if the data is unimodal, bimodal, or skewed—helping to identify outliers or multiple populations in the data.\n",
    "- **Better Comparison:** When comparing multiple distributions, KDE lines provide a clearer, direct comparison of density and shape than overlapping histograms.\n",
    "\n",
    "In summary, adding a KDE line to a histogram makes your data visualization more informative, especially when diagnosing patterns or anomalies in numeric data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778dc8cc-9ada-4e66-88ed-de3f16ff7fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = df.corr(numeric_only=True)\n",
    "\n",
    "fig = px.imshow(\n",
    "    corr,\n",
    "    text_auto=\".2f\",  # Automatically annotate with correlation value\n",
    "    color_continuous_scale='Viridis',\n",
    "    aspect='auto',\n",
    "    labels=dict(x=\"Variable\", y=\"Variable\", color=\"Correlation\"),\n",
    "    title=\"Correlation Heatmap (Interactive)\"\n",
    ")\n",
    "\n",
    "# Enhance layout and interactivity\n",
    "fig.update_layout(\n",
    "    width=900,\n",
    "    height=900,\n",
    "    title_x=0.5,\n",
    "    font=dict(size=16),\n",
    "    xaxis=dict(tickangle=45),\n",
    ")\n",
    "\n",
    "# Optionally make the annotation text color white for higher contrast on dark backgrounds\n",
    "fig.update_traces(\n",
    "    hovertemplate=\"<b>%{x}</b> vs <b>%{y}</b><br>Correlation: %{z:.2f}<extra></extra>\",\n",
    "    textfont_size=16,\n",
    "    textfont_color=\"white\"\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233060f2-a0d2-418c-8332-8271417587cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Fit linear regression model with all features\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "print(\"Intercept (beta_0):\", model.intercept_)\n",
    "print(\"Coefficients (beta_j):\")\n",
    "for name, coef in zip(X.columns, model.coef_):\n",
    "    print(f\"{name}: {coef:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aec0c17-2fb1-49a0-86c3-52347282486d",
   "metadata": {},
   "source": [
    "### Understanding the Slope and Intercept in Multiple Regression\n",
    "\n",
    "In a multiple regression model, the prediction equation takes the form:\n",
    "\n",
    "$$\n",
    "\\text{MedHouseVal} = \\beta_0 + \\beta_1 \\cdot \\text{MedInc} + \\beta_2 \\cdot \\text{HouseAge} + \\cdots + \\beta_8 \\cdot \\text{Longitude}\n",
    "$$\n",
    "\n",
    "**Intercept ($\\beta_0$):**\n",
    "- The intercept is the predicted value of the target variable ($y$) when all features ($x_1, x_2, ..., x_n$) are set to zero.\n",
    "- In our California housing model:\n",
    "  - $\\beta_0 = -36.94$\n",
    "- Interpretation: If all input features were zero, the model predicts a median house value of $-36.94 \\times \\$100,000$, which is not realistic in practice but mathematically defines the baseline for the regression line.\n",
    "\n",
    "**Slope ($\\beta_j$ for each variable):**\n",
    "\n",
    "Each coefficient ($\\beta_j$) represents the **change in the target ($y$)** for a **one-unit increase** in that feature, *holding all other features constant*.\n",
    "\n",
    "For example, in our results:\n",
    "\n",
    "- $0.4367$ for **MedInc**: For each additional unit increase in median income (in $10,000s$), the predicted median house value increases by $0.4367 \\times \\$100,000$ ($43,670)$, keeping all other variables constant.\n",
    "- $0.0094$ for **HouseAge**: For each extra year of average house age, the predicted house value increases by about $940$.\n",
    "- $-0.1073$ for **AveRooms**: For each additional average room, the predicted value decreases by about $-10,730$.\n",
    "- $0.6451$ for **AveBedrms**: For each additional average bedroom, the predicted value increases by about $64,510$.\n",
    "- $-0.0000$ for **Population**: For each extra person, the effect on predicted value is negligible.\n",
    "- $-0.0038$ for **AveOccup**: For each additional person per household, the predicted value decreases by about $-380$.\n",
    "- $-0.4213$ for **Latitude**: For each degree further north, the predicted value decreases by about $-42,130$.\n",
    "- $-0.4345$ for **Longitude**: For each degree further east, the predicted value decreases by about $-43,450$.\n",
    "\n",
    "...and similarly for any other coefficients.\n",
    "\n",
    "\n",
    "**Why the Means Matter:**\n",
    "- The regression line (or hyperplane) is positioned based on the means of $x$ and $y$. If you compute predictions at the mean values of all predictors, the result should be close to the mean of $y$ (the average house value in the dataset).\n",
    "- The intercept itself is often not meaningful in real-world terms if no homes have all features at zero, but it is essential for accurate predictions and for anchoring the regression model.\n",
    "\n",
    "**Summary Table from the Model:**\n",
    "\n",
    "| Feature    | Coefficient ($\\beta_j$) | Interpretation Example |\n",
    "|------------|--------------------------|----------------------|\n",
    "| Intercept  | -36.94                   | Baseline prediction when all features are zero |\n",
    "| MedInc     | 0.4367                   | +1 unit MedInc $\\Rightarrow$ +$43,670$ |\n",
    "| HouseAge   | 0.0094                   | +1 year $\\Rightarrow$ +$940$ |\n",
    "| AveRooms   | -0.1073                  | +1 room $\\Rightarrow$ -$10,730$ |\n",
    "| AveBedrms  | 0.6451                   | +1 bedroom $\\Rightarrow$ +$64,510$ |\n",
    "| Population | -0.0000                  | +1 person $\\Rightarrow$ negligible effect |\n",
    "| AveOccup   | -0.0038                  | +1 person/household $\\Rightarrow$ -$380$ |\n",
    "| Latitude   | -0.4213                  | +1 degree north $\\Rightarrow$ -$42,130$ |\n",
    "| Longitude  | -0.4345                  | +1 degree east $\\Rightarrow$ -$43,450$ |\n",
    "\n",
    "---\n",
    "\n",
    "In multiple regression, **each slope quantifies the unique contribution of that variable to the prediction, controlling for all others**. The intercept is a mathematical anchor; the means show where your model is centered in the feature space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d8d114-4b2d-4d66-bd51-ee1e6a9acae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the target values\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Create a DataFrame with all features, actual, and predicted values\n",
    "results_df = X.copy()\n",
    "results_df['Actual'] = y\n",
    "results_df['Predicted'] = y_pred\n",
    "\n",
    "# Show the first 10 rows\n",
    "results_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fa91b2-b910-4762-bc91-432244dff7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Compute residuals\n",
    "results_df['Residual'] = results_df['Actual'] - results_df['Predicted']\n",
    "\n",
    "# Interactive residuals scatter plot\n",
    "fig = px.scatter(\n",
    "    results_df, \n",
    "    x='Predicted', y='Residual',\n",
    "    hover_data=results_df.columns,\n",
    "    color='Residual',\n",
    "    color_continuous_scale='RdBu',\n",
    "    title='Residuals vs. Predicted Values',\n",
    "    labels={\n",
    "        'Predicted': 'Predicted Median House Value ($100,000s)',\n",
    "        'Residual': 'Residual (Actual - Predicted)'\n",
    "    },\n",
    "    width=800,\n",
    "    height=500\n",
    ")\n",
    "fig.add_hline(y=0, line_dash=\"dash\", line_color=\"black\")\n",
    "fig.update_traces(marker=dict(size=7, line=dict(width=1, color='black')))\n",
    "fig.update_layout(\n",
    "    title_x=0.5,\n",
    "    font=dict(size=16),\n",
    "    xaxis=dict(showgrid=True),\n",
    "    yaxis=dict(showgrid=True)\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0b335b-2572-4f30-a21e-d8173825f10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# You can pick the two most important features for X and Y axes\n",
    "x_feature = 'MedInc'\n",
    "y_feature = 'AveRooms'\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=results_df[x_feature],\n",
    "    y=results_df[y_feature],\n",
    "    z=results_df['Actual'],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=4,\n",
    "        color=results_df['Residual'],         # Color by residual value\n",
    "        colorscale='RdBu',\n",
    "        colorbar=dict(title=\"Residual\"),\n",
    "        opacity=0.85,\n",
    "        line=dict(width=0.5, color='black')\n",
    "    ),\n",
    "    text=[f\"{x_feature}: {x:.2f}<br>{y_feature}: {y:.2f}<br>Actual: {a:.2f}<br>Predicted: {p:.2f}<br>Residual: {r:.2f}\"\n",
    "          for x, y, a, p, r in zip(\n",
    "              results_df[x_feature], \n",
    "              results_df[y_feature], \n",
    "              results_df['Actual'], \n",
    "              results_df['Predicted'], \n",
    "              results_df['Residual']\n",
    "          )],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    scene = dict(\n",
    "        xaxis_title=x_feature,\n",
    "        yaxis_title=y_feature,\n",
    "        zaxis_title='Actual Median House Value',\n",
    "    ),\n",
    "    title=f\"3D Scatter: {x_feature}, {y_feature}, Actual Value (Colored by Residual)\",\n",
    "    width=900,\n",
    "    height=700,\n",
    "    title_x=0.5,\n",
    "    margin=dict(l=0, r=0, b=0, t=60)\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65667d37-2e35-432e-83d3-aa37f972e80a",
   "metadata": {},
   "source": [
    "### Model Evaluation Metrics: How to Assess Your Regression Model\n",
    "\n",
    "Evaluating a regression model requires a deep understanding of multiple metrics, each serving a unique role in diagnosing performance.\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Sum of Squared Errors (SSE)**\n",
    "\n",
    "**Definition:**  \n",
    "The sum of the squares of the residuals (errors) between actual ($y_i$) and predicted ($\\hat{y}_i$) values.\n",
    "\n",
    "**Equation:**  \n",
    "$$\n",
    "\\mathrm{SSE} = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "**How to Calculate:**  \n",
    "- For every data point, compute the difference between actual and predicted value.\n",
    "- Square each difference.\n",
    "- Sum all squared differences.\n",
    "\n",
    "**When to Use:**  \n",
    "- Useful for measuring the total error of the model.\n",
    "- Not comparable across datasets of different sizes (grows with $n$).\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Mean Squared Error (MSE)**\n",
    "\n",
    "**Definition:**  \n",
    "The average of squared errors across all data points.\n",
    "\n",
    "**Equation:**  \n",
    "$$\n",
    "\\mathrm{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "**How to Calculate:**  \n",
    "- Compute SSE.\n",
    "- Divide by the number of observations $n$.\n",
    "\n",
    "**When to Use:**  \n",
    "- Standard for comparing regression models on the same dataset.\n",
    "- Penalizes large errors more than small ones due to squaring.\n",
    "- Same unit as the square of the target variable.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Root Mean Squared Error (RMSE)**\n",
    "\n",
    "**Definition:**  \n",
    "The square root of MSE, bringing the error measure back to the original unit of $y$.\n",
    "\n",
    "**Equation:**  \n",
    "$$\n",
    "\\mathrm{RMSE} = \\sqrt{\\mathrm{MSE}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\n",
    "$$\n",
    "\n",
    "**How to Calculate:**  \n",
    "- Take the square root of the MSE.\n",
    "\n",
    "**When to Use:**  \n",
    "- Directly interpretable in the same units as the target variable.\n",
    "- Useful for comparing model error to the scale of $y$.\n",
    "- Sensitive to outliers.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Mean Absolute Error (MAE)**\n",
    "\n",
    "**Definition:**  \n",
    "The average of the absolute differences between actual and predicted values.\n",
    "\n",
    "**Equation:**  \n",
    "$$\n",
    "\\mathrm{MAE} = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "**How to Calculate:**  \n",
    "- Compute the absolute value of each residual.\n",
    "- Take the average over all data points.\n",
    "\n",
    "**When to Use:**  \n",
    "- Less sensitive to large outliers than MSE/RMSE.\n",
    "- Useful when you care about average magnitude of errors, not their direction.\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. R-squared ($R^2$) — Coefficient of Determination**\n",
    "\n",
    "**Definition:**  \n",
    "The proportion of the variance in the dependent variable explained by the regression model.\n",
    "\n",
    "**Equation:**  \n",
    "$$\n",
    "R^2 = 1 - \\frac{\\mathrm{SSE}}{\\mathrm{SST}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mathrm{SSE}$ is the sum of squared errors as above.\n",
    "- $\\mathrm{SST} = \\sum_{i=1}^n (y_i - \\bar{y})^2$ is the total sum of squares, measuring total variance in $y$.\n",
    "\n",
    "**How to Calculate:**  \n",
    "- Compute SSE and SST.\n",
    "- Take $1 - (\\mathrm{SSE}/\\mathrm{SST})$.\n",
    "\n",
    "**When to Use:**  \n",
    "- $R^2$ ranges from $0$ to $1$ (can be negative for a model worse than mean prediction).\n",
    "- Closer to $1$ means a better fit.\n",
    "- Use for assessing model’s explanatory power.\n",
    "- Be careful: A high $R^2$ does not guarantee a good model (overfitting, omitted variable bias, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "### **How to Choose the Right Metric**\n",
    "\n",
    "- **MAE**: Use for interpretability and when outliers are not as important.\n",
    "- **MSE/RMSE**: Use when you want to heavily penalize large errors or when errors need to be in squared units.\n",
    "- **SSE**: Diagnostic, but best for comparing fits on the same dataset.\n",
    "- **$R^2$**: Use for a quick sense of explained variance, but always in conjunction with error metrics.\n",
    "\n",
    "**Key Practice:**  \n",
    "- Always report multiple metrics.\n",
    "- Interpret them in the context of your data’s scale, distribution, and the problem at hand.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dad1166-5604-4f7e-84bd-127aea4d6f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Actual and predicted values\n",
    "y_true = results_df['Actual']\n",
    "y_pred = results_df['Predicted']\n",
    "\n",
    "# Sum of Squared Errors (SSE)\n",
    "sse = np.sum((y_true - y_pred) ** 2)\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "\n",
    "# Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "# Total Sum of Squares (SST)\n",
    "sst = np.sum((y_true - y_true.mean()) ** 2)\n",
    "\n",
    "# R-squared\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Sum of Squared Errors (SSE): {sse:.2f}\")\n",
    "print(f\"Total Sum of Squares (SST): {sst:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"R-squared (R^2): {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73db5c01-d3da-4d9d-88d3-c2097c3741c6",
   "metadata": {},
   "source": [
    "### Interpretation of Regression Metrics\n",
    "\n",
    "Based on your model's output:\n",
    "\n",
    "- **Sum of Squared Errors (SSE): 10,821.99**\n",
    "  - This represents the total squared deviation of the predicted values from the actual median house values.\n",
    "  - While the absolute number is dataset-dependent, a lower SSE always indicates a closer fit of the model to the data. Here, the model leaves about 10,822 units of error unexplained by its predictions.\n",
    "\n",
    "- **Total Sum of Squares (SST): 27,483.20**\n",
    "  - This is the total variance present in the actual data, i.e., the sum of squared differences between the observed values and the mean of the observed values.\n",
    "  - SST sets a benchmark for how much the data can be explained by any model (including just predicting the mean).\n",
    "\n",
    "- **Mean Squared Error (MSE): 0.5243**\n",
    "  - The average squared error per prediction.\n",
    "  - This means, on average, each prediction is off by about $0.5243$ (in the same units as your target variable squared). For California housing, the units are in $100,000s$, so this corresponds to $52,430$ squared error per home.\n",
    "\n",
    "- **Root Mean Squared Error (RMSE): 0.7241**\n",
    "  - The square root of MSE, bringing error units back to the original scale.\n",
    "  - The average magnitude of error for each prediction is roughly $0.7241$ ($72,410)$.\n",
    "  - This gives you a sense of the \"typical\" prediction error: the model’s predicted median house value will usually be within about $72,410 of the true value.\n",
    "\n",
    "- **Mean Absolute Error (MAE): 0.5312**\n",
    "  - The mean of the absolute differences between predicted and actual values.\n",
    "  - On average, your predictions are off by about $0.5312$ ($53,120) per home.\n",
    "  - MAE is robust to outliers and offers a straightforward, intuitive measure of typical error.\n",
    "\n",
    "- **R-squared ($R^2$): 0.6062**\n",
    "  - This means that approximately 60.6% of the variance in median house value across California is explained by your linear regression model.\n",
    "  - In other words, your features capture a substantial but not exhaustive amount of the underlying patterns in the housing prices. There’s still ~39.4% of the variance that your model does not explain, potentially due to non-linearities, omitted features, or inherent randomness.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Summary and Recommendations:**\n",
    "- **Model Quality:** Your model does a reasonable job—capturing over 60% of the explainable variance and producing a typical prediction error just above $50,000$. This is a strong start for a purely linear approach with default features.\n",
    "- **Diagnostic Use:** \n",
    "  - *RMSE* and *MAE* should always be interpreted relative to the natural variability (SST) and the range of the target variable.\n",
    "  - If the average house value is, for example, $2.07 ($207,000), an RMSE of $0.72 ($72,410) suggests there is still significant room for improvement.\n",
    "- **Next Steps:**\n",
    "  - Investigate residuals for patterns (non-linearity, heteroscedasticity).\n",
    "  - Try more complex models (e.g., polynomial regression, decision trees, random forests).\n",
    "  - Feature engineering: add or transform variables, or include location-based features with higher resolution.\n",
    "  - Cross-validate to ensure generalizability to unseen data.\n",
    "\n",
    "**Bottom line:**  \n",
    "This model is interpretable and captures much of the important variation in California housing prices, but there’s scope to reduce error further by refining the model or incorporating richer data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb4d0d4-265f-4ae6-97c2-567e2f439371",
   "metadata": {},
   "source": [
    "### Next Steps: Beyond Basic Regression\n",
    "\n",
    "So far, we have:\n",
    "- Built and evaluated a linear regression model using all available data.\n",
    "\n",
    "However, two important topics remain for building robust, real-world predictive models:\n",
    "\n",
    "---\n",
    "\n",
    "**1. Train/Test Split**\n",
    "\n",
    "In practice, we want to assess how well our model generalizes to new, unseen data.  \n",
    "To do this, we divide our data into:\n",
    "- **Training set:** Used to fit the model.\n",
    "- **Test set:** Used only to evaluate model performance.\n",
    "\n",
    "This helps us detect overfitting (when a model learns the training data too well but performs poorly on new data).\n",
    "\n",
    "---\n",
    "\n",
    "**2. Regularization**\n",
    "\n",
    "With multiple predictors, models can become too flexible and start fitting noise rather than true relationships—a problem known as overfitting.  \n",
    "**Regularization** adds a penalty to the model for having large or complex coefficients, encouraging simpler, more generalizable solutions.\n",
    "\n",
    "Two common types:\n",
    "- **Ridge Regression (L2):** Penalizes the sum of squared coefficients.\n",
    "- **Lasso Regression (L1):** Penalizes the sum of absolute coefficients and can set some coefficients to zero (feature selection).\n",
    "\n",
    "---\n",
    "\n",
    "**In the next steps, we will:**\n",
    "1. Split the data into training and test sets.\n",
    "2. Fit and evaluate models with and without regularization (ridge and lasso).\n",
    "3. Compare the results to see how regularization helps prevent overfitting and improves generalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd4553f-3672-4b3c-9950-7013a0f3d641",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Train/Test Split (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Fit Models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0),\n",
    "    'Lasso Regression': Lasso(alpha=0.1)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'SSE': np.sum((y_test - y_pred)**2),\n",
    "        'MSE': mean_squared_error(y_test, y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "        'MAE': mean_absolute_error(y_test, y_pred),\n",
    "        'R^2': r2_score(y_test, y_pred)\n",
    "    })\n",
    "\n",
    "# 3. Results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df[['Model', 'SSE', 'MSE', 'RMSE', 'MAE', 'R^2']]\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c563336-71b4-4350-a712-3c732e858f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for all models on the test set\n",
    "y_pred_lin = models['Linear Regression'].predict(X_test)\n",
    "y_pred_ridge = models['Ridge Regression'].predict(X_test)\n",
    "y_pred_lasso = models['Lasso Regression'].predict(X_test)\n",
    "\n",
    "# Build a DataFrame with all features, actual values, and all model predictions\n",
    "compare_df = X_test.copy()\n",
    "compare_df['Actual'] = y_test\n",
    "compare_df['Linear_Pred'] = y_pred_lin\n",
    "compare_df['Ridge_Pred'] = y_pred_ridge\n",
    "compare_df['Lasso_Pred'] = y_pred_lasso\n",
    "\n",
    "# Show the first 10 rows for inspection\n",
    "compare_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a112f5-9503-4816-af60-209998138c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt user for input with instructions on data type\n",
    "\n",
    "print(\"Please enter values for the following features:\")\n",
    "\n",
    "medinc = float(input(\"Median Income (MedInc, e.g. 4.5) [float]: \"))\n",
    "houseage = int(input(\"House Age (HouseAge, e.g. 30) [int]: \"))\n",
    "averooms = float(input(\"Average Rooms (AveRooms, e.g. 6.0) [float]: \"))\n",
    "avebedrms = float(input(\"Average Bedrooms (AveBedrms, e.g. 1.0) [float]: \"))\n",
    "population = int(input(\"Population (e.g. 1000) [int]: \"))\n",
    "aveoccup = float(input(\"Average Occupancy (AveOccup, e.g. 3.0) [float]: \"))\n",
    "latitude = float(input(\"Latitude (e.g. 34.0) [float]: \"))\n",
    "longitude = float(input(\"Longitude (e.g. -118.0) [float]: \"))\n",
    "\n",
    "# Assemble input into a DataFrame\n",
    "user_sample = pd.DataFrame([{\n",
    "    'MedInc': medinc,\n",
    "    'HouseAge': houseage,\n",
    "    'AveRooms': averooms,\n",
    "    'AveBedrms': avebedrms,\n",
    "    'Population': population,\n",
    "    'AveOccup': aveoccup,\n",
    "    'Latitude': latitude,\n",
    "    'Longitude': longitude\n",
    "}])\n",
    "\n",
    "# Predict with all trained models\n",
    "lin_pred = models['Linear Regression'].predict(user_sample)[0]\n",
    "ridge_pred = models['Ridge Regression'].predict(user_sample)[0]\n",
    "lasso_pred = models['Lasso Regression'].predict(user_sample)[0]\n",
    "\n",
    "print(f\"\\nPredicted Median House Value (Linear Regression): ${lin_pred * 100000:,.2f}\")\n",
    "print(f\"Predicted Median House Value (Ridge Regression):  ${ridge_pred * 100000:,.2f}\")\n",
    "print(f\"Predicted Median House Value (Lasso Regression):  ${lasso_pred * 100000:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0a6b90-c464-48f3-ab77-b13954da5b78",
   "metadata": {},
   "source": [
    "### Final Interpretation and Reflection\n",
    "\n",
    "#### **Comparing Model Performance**\n",
    "\n",
    "Based on the test set results:\n",
    "\n",
    "| Model              | SSE      | MSE     | RMSE    | MAE     | $R^2$   |\n",
    "|--------------------|----------|---------|---------|---------|---------|\n",
    "| Linear Regression  | 2294.72  | 0.5559  | 0.7456  | 0.5332  | 0.5758  |\n",
    "| Ridge Regression   | 2294.36  | 0.5558  | 0.7455  | 0.5332  | 0.5759  |\n",
    "| Lasso Regression   | 2532.58  | 0.6135  | 0.7833  | 0.5816  | 0.5318  |\n",
    "\n",
    "- **Linear and Ridge regression** perform almost identically in this case, with Ridge offering slightly better $R^2$ and error metrics—indicating that the model benefits only a little from regularization, likely due to moderate feature correlations and the scale of coefficients.\n",
    "- **Lasso regression** produces a higher error and lower $R^2$. This suggests that, at the chosen penalty, setting some coefficients closer to zero actually reduces model performance for this dataset. Lasso is most useful when strong feature selection is needed (sparse model), which may not be the case here.\n",
    "- All models explain roughly **57–58% of the variance** in California house prices in the test set. This leaves about 42% unexplained, pointing to possible non-linear patterns, omitted variables, or noise.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Interpreting Individual Predictions**\n",
    "\n",
    "Sample rows show that even with the best model, individual predictions can still be far from the actual value, especially at the extremes or for outlier data points. For example, the first test example had an actual value of $0.477$ ($\\$47,700$) but the predictions ranged from $0.719$ ($\\$71,900$) to $1.046$ ($\\$104,600$).\n",
    "\n",
    "---\n",
    "\n",
    "#### **Reference to the Book**\n",
    "\n",
    "As presented in *Data Science from Scratch (Chapters 14–15)*, the core workflow of regression modeling includes:\n",
    "- Splitting data to assess generalization,\n",
    "- Fitting models with and without regularization,\n",
    "- Interpreting both coefficients and prediction errors,\n",
    "- Using evaluation metrics ($R^2$, MAE, MSE, RMSE) to understand both fit and predictive quality,\n",
    "- Recognizing limitations of linear models and when more flexible or sophisticated approaches may be necessary.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Something to Think About**\n",
    "\n",
    "> **Are we reaching the limit of what a linear model can do with these features?**\n",
    ">\n",
    "> The gap between actual and predicted values, and the $R^2$ ceiling, suggest opportunities for:\n",
    "> - Adding new, more informative features (feature engineering)\n",
    "> - Using non-linear models (decision trees, ensembles, or neural nets)\n",
    "> - Further exploring regularization and hyperparameter tuning\n",
    "\n",
    "**Reflect:** What real-world processes or data limitations could account for the unexplained variance in house prices?  \n",
    "What trade-offs exist between interpretability and predictive power in model selection?\n",
    "\n",
    "---\n",
    "\n",
    "**For next time:** Consider how you would approach this problem if model *explainability* was crucial (for a policymaker or a homeowner) versus if you only cared about raw predictive performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9be028-c9f0-4239-87a8-99ff385bbe5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf856d83-a006-46dd-9af1-bb6dbd68a933",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
